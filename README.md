### Welcome! ðŸ‘‹

My name is Yihong Chen. I research on AI knowledge acquisition, specifically on how different AI systems can learn to abstract, represent, and use concepts/symbols efficiently.

I am open to collaborations on topics related to embedding learning, link prediction, and language modeling. If you would like to get in touch, you can reach me by emailing yihong-chen AT outlook DOT com.

### Looking for Some Inspirations?

ðŸ’¥ Mar 2025, we have a new tool for interpreting transformers, check out [jet expansion](https://yihong-chen.github.io/jet_expand/).

ðŸ’¥ Mar 2024, Quanta Magazine covers our research on periodic embedding forgetting. Check out the article [here](https://www.quantamagazine.org/how-selective-forgetting-can-help-ai-learn-better-20240228/). 

ðŸ’¥ Dec 2023, I will present our forgetting paper at NeurIPS 2023. Check out the poster [here](https://pbs.twimg.com/media/GBQ6AGBWAAA6sn4?format=jpg&name=4096x4096).

:boom: Sep 2023, our latest work [*Improving Language Plasticity via Pretraining with Active Forgetting*](https://arxiv.org/abs/2307.01163) is accepted by NeurIPS 2023!

:boom: Sep 2023, I presented our latest work on forgetting at [IST-Unbabel seminar](https://ist-unbabel-seminars.github.io/).

:boom: Jul 2023, I presented our latest work on forgetting in language modeling at ELLIS Unconference 2023. The slides are available [here](https://docs.google.com/presentation/d/16JMv3_P9w0kX7NXkvN73236atOjwWXWiLp1anKOubxo/edit?usp=sharing). Feel free to leave your comments.

:boom: Jul 2023, discover the power of forgetting in language modeling! Our latest work, [*Improving Language Plasticity via Pretraining with Active Forgetting*](https://arxiv.org/abs/2307.01163), shows how pretraining a language model with active forgetting can help it quickly learn **new** languages. You'll be amazed by the model plasticity imbued via pretraining with forgetting. Check it out :)

:boom: Nov 2022, our paper, *[REFACTOR GNNS: Revisiting Factorisation-based Models from a Message-Passing Perspective](https://arxiv.org/pdf/2207.09980.pdf)*, will appear in NeurIPS 2022! If you're interested in understanding why FMs can be some special GNNs and make them usable on **new** graphs, check it out!

:boom: Jun 2022, if you're looking for a hands-on repo to start experimenting with link prediction, check out our repo [ssl-relation-prediction](https://github.com/facebookresearch/ssl-relation-prediction). Simple code, easy to hack ðŸš€

