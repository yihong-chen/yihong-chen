### Welcome! ðŸ‘‹

My name is Yihong Chen, and I am a joint PhD student between Meta AI (FAIR London) and [UCL NLP](https://nlp.cs.ucl.ac.uk/). My research focuses on AI knowledge acquisition, specifically on how different AI systems can learn to represent and use concepts/symbols efficiecntly.

I am open to collaborations on topics related to embedding learning, link prediction, and language modeling. If you would like to get in touch, you can reach me by emailing yihong-chen AT outlook DOT com, poke me on [Threads](https://www.threads.net/@yihong.curiosity) or simply booking a [Zoom meeting](https://calendly.com/yhc-curiosity/30min) with me.

### Looking for Some Inspirations?

:boom: Jul 2023, I presented our latest work on forgetting language modelling at ELLIS Unconference 2023. The slides are available [here](https://docs.google.com/presentation/d/16JMv3_P9w0kX7NXkvN73236atOjwWXWiLp1anKOubxo/edit?usp=sharing). Feel free to leave your comments.

:boom: Jul 2023, discover the power of forgetting in language modelling! Our latest work, [*Improving Language Plasticity via Pretraining with Active Forgetting*](https://arxiv.org/abs/2307.01163), shows how pretraining a language model with active forgetting can help it quickly learn **new** languages. You'll be amazed by the model plasticity imbued via pretraining with forgetting. Check it out :)

:boom: Nov 2022, our paper, *[REFACTOR GNNS: Revisiting Factorisation-based Models from a Message-Passing Perspective](https://arxiv.org/pdf/2207.09980.pdf)*, will appear in NeurIPS 2022! If you're interested in understanding why FMs can be some special GNNs and make them usable on **new** graphs, check it out!

:boom: Jun 2022, if you're looking for a hands-on repo to start experimenting with link prediction, check out our repo [ssl-relation-prediction](https://github.com/facebookresearch/ssl-relation-prediction). Simple code, easy to hack ðŸš€

