### Welcome! ðŸ‘‹

My name is Yihong Chen, a researcher affiliated with OATML. I research on AI knowledge acquisition, specifically on how different AI systems can learn to abstract, represent, and use concepts/symbols efficiently. Particularly I care about bridging the gap between structured AI paradigm (e.g. symbolic AI) and unstructured AI paradigm (e.g. large language models) so that we can have a unified understanding of how knowledge formation happens in intelligent units. My thesis articulates the [vision](https://yihong-chen.github.io/thesis/). 

With this vision in mind, my recent focus is on AI reasoning, memory and continual learning. If you would like to get in touch, you can reach me at yihong-chen AT outlook DOT com. If you are a student at Oxford, please see my professional page [here](https://www.cs.ox.ac.uk/teaching/courses/projects/) for opportunities to get involved.

### Looking for Some Inspirations?

:boom: Jul 2025, [My PhD thesis is out!](https://yihong-chen.github.io/thesis/) Knowledge engines need not just structure, but also destructuring â€” for plasticity, flow, and adaptability.

ðŸ’¥ Mar 2025, We have a new tool for interpreting transformers, check out [jet expansion](https://yihong-chen.github.io/jet_expand/).

ðŸ’¥ Mar 2024, Quanta Magazine covers our research on [active forgetting](https://yihong-chen.github.io/active-forgetting/). Check out the article [here](https://www.quantamagazine.org/how-selective-forgetting-can-help-ai-learn-better-20240228/).

ðŸ’¥ Dec 2023, I will present our forgetting paper at NeurIPS 2023. Check out the poster [here](https://pbs.twimg.com/media/GBQ6AGBWAAA6sn4?format=jpg&name=4096x4096)!

:boom: Sep 2023, I presented our latest work on forgetting at [IST-Unbabel seminar](https://ist-unbabel-seminars.github.io/).

:boom: Jul 2023, I presented our latest work on forgetting in language modeling at ELLIS Unconference 2023. The slides are available [here](https://docs.google.com/presentation/d/16JMv3_P9w0kX7NXkvN73236atOjwWXWiLp1anKOubxo/edit?usp=sharing). Feel free to leave your comments.

:boom: Jul 2023, discover the power of forgetting in language modeling! Our latest work, [*Improving Language Plasticity via Pretraining with Active Forgetting*](https://arxiv.org/abs/2307.01163), shows how pretraining a language model with active forgetting can help it quickly learn **new** languages. You'll be amazed by the model plasticity imbued via pretraining with forgetting. Check it out :)

:boom: Nov 2022, our paper, *[REFACTOR GNNS: Revisiting Factorisation-based Models from a Message-Passing Perspective](https://arxiv.org/pdf/2207.09980.pdf)*, will appear in NeurIPS 2022! If you're interested in understanding why FMs can be some special GNNs and make them usable on **new** graphs, check it out!

:boom: Jun 2022, if you're looking for a hands-on repo to start experimenting with link prediction, check out our repo [ssl-relation-prediction](https://github.com/facebookresearch/ssl-relation-prediction). Simple code, easy to hack ðŸš€

